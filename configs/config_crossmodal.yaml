training:
  n_epochs: 50
  batch_size: 8
  learning_rate: 0.001
  device: "cuda"
  experiment_name: "with_cross_modal_attention"

model:
  latent_dim: 256
  embedding_dim: 768
  text_hidden_dim: 256
  gru_hidden_dim: 256
  num_layers: 2
  dropout: 0.2
  use_cross_modal_attention: true
  use_vae_decoder: false

data:
  image_height: 60
  image_width: 125
  text_max_length: 120
  train_split: 0.8

loss:
  image_weight: 0.2 
  text_weight: 10.0   
  context_weight: 1
  kl_weight: 0.0

paths:
  project_root: "/content/gdrive/MyDrive/project_eniola-ak"
  checkpoint_dir: "checkpoints/cross_modal_attention"
  results_dir: "results/cross_modal_attention"

logging:
  save_every_n_epochs: 25
  save_visualizations: true