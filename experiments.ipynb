{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOj9b8s2owlK"
   },
   "source": [
    "![banner.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMoDtdJ_o5-y"
   },
   "source": [
    "# Final assessment - Main Notebook\n",
    "---\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "In this notebook you will find a baseline, functional architecture for your assessment task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21506,
     "status": "ok",
     "timestamp": 1764070515944,
     "user": {
      "displayName": "ENIOLA AKINLUA",
      "userId": "13807007961803944509"
     },
     "user_tz": 0
    },
    "id": "ynhF-mN7XNpl"
   },
   "outputs": [],
   "source": [
    "# @title Importing the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from datasets.fingerprint import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import gc\n",
    "\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Configuration for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "PROJECT_ROOT = Path('/content/gdrive/MyDrive/project_eniola-ak')\n",
    "\n",
    "CONFIG_NAME = 'config.yaml'\n",
    "# CONFIG_NAME = 'configs/config_baseline.yaml'\n",
    "# CONFIG_NAME = 'configs/config_crossmodal.yaml'\n",
    "# CONFIG_NAME = 'configs/config_vae.yaml'\n",
    "# CONFIG_NAME = 'configs/config_both.yaml'\n",
    "\n",
    "config_path = PROJECT_ROOT / CONFIG_NAME\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "  config = yaml.safe_load(f)\n",
    "\n",
    "N_EPOCHS = config['training']['n_epochs']\n",
    "BATCH_SIZE = config['training']['batch_size']\n",
    "LEARNING_RATE = config['training']['learning_rate']\n",
    "EXPERIMENT_NAME = config['training']['experiment_name']\n",
    "\n",
    "LATENT_DIM = config['model']['latent_dim']\n",
    "EMBEDDING_DIM = config['model']['embedding_dim']\n",
    "GRU_HIDDEN_DIM = config['model']['gru_hidden_dim']\n",
    "NUM_LAYERS = config['model']['num_layers']\n",
    "DROPOUT = config['model']['dropout']\n",
    "USE_CROSS_MODAL= config['model']['use_cross_modal_attention']\n",
    "USE_VAE = config['model']['use_vae_decoder']\n",
    "\n",
    "LOSS_IMG_W = config['loss']['image_weight']\n",
    "LOSS_TEXT_W = config['loss']['text_weight']\n",
    "LOSS_CTX_W = config['loss']['context_weight']\n",
    "KL_WEIGHT = config['loss']['kl_weight']\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT / config['paths']['results_dir'] / EXPERIMENT_NAME\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / config['paths']['checkpoint_dir']\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2HPgWOUpkby"
   },
   "source": [
    "# **Chapter 1: The data preparation**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-RwDxeFbVRH"
   },
   "source": [
    "First we need to activate our google drive so that we can save out data permanently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TePRpTocsB7J"
   },
   "source": [
    "## 1.1 Loading and saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15821,
     "status": "ok",
     "timestamp": 1764070532145,
     "user": {
      "displayName": "ENIOLA AKINLUA",
      "userId": "13807007961803944509"
     },
     "user_tz": 0
    },
    "id": "oQiCMmbedPBt",
    "outputId": "735d5a8f-f736-4492-a273-49a20b3228cf"
   },
   "outputs": [],
   "source": [
    "# @title Setting up google drive to save checkpoints\n",
    "\n",
    "# This will prompt you to authorize Google Drive access\n",
    "def save_checkpoint_to_drive(model, optimizer, epoch, loss, filename=\"autoencoder_checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Saves the checkpoint directly to a specified folder in your mounted Google Drive.\n",
    "    \"\"\"\n",
    "    # 1. Define the full Google Drive path\n",
    "    # 'DL_Checkpoints' is the folder you want to save to inside your Drive\n",
    "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
    "\n",
    "    # Ensure the directory exists before attempting to save\n",
    "    os.makedirs(drive_folder, exist_ok=True)\n",
    "\n",
    "    # 2. Combine the folder and the filename\n",
    "    full_path = os.path.join(drive_folder, filename)\n",
    "\n",
    "    # 3. Create the checkpoint dictionary\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # 4. Save the dictionary to the Google Drive path\n",
    "    torch.save(checkpoint, full_path)\n",
    "    print(f\"Checkpoint saved to Google Drive: {full_path} at epoch {epoch}\")\n",
    "\n",
    "\n",
    "def load_checkpoint_from_drive(model, optimizer=None, filename=\"autoencoder_checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint from your Google Drive folder into the model and optimizer (if provided).\n",
    "    \"\"\"\n",
    "    # Define the same Google Drive folder path\n",
    "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
    "    full_path = os.path.join(drive_folder, filename)\n",
    "\n",
    "    # Check if the checkpoint file exists\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {full_path}\")\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(full_path, map_location=torch.device('cpu'))  # use cuda if available\n",
    "\n",
    "    # Restore model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Restore optimizer state (if provided)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Extract metadata\n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    loss = checkpoint.get('loss', None)\n",
    "\n",
    "    print(f\"Checkpoint loaded from: {full_path} (epoch {epoch})\")\n",
    "\n",
    "    return model, optimizer, epoch, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfZ2gtY9b_HW"
   },
   "source": [
    "We need to define a couple of functions to make our life easier. Feel free to tweak those functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU6HfA99zqD_"
   },
   "outputs": [],
   "source": [
    "# @title Functions to load images and process data\n",
    "\n",
    "\n",
    "# This function just extracts the tags from the text, don't get distracted by it.\n",
    "# I changed this function a bit to fix some bugs\n",
    "def parse_gdi_text(text):\n",
    "    \"\"\"Parse GDI formatted text into structured data\"\"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    images = []\n",
    "\n",
    "    for gdi in soup.find_all('gdi'):\n",
    "        # Debug: print what BeautifulSoup sees\n",
    "\n",
    "        # Method 1: Try to get image attribute directly\n",
    "        image_id = None\n",
    "        if gdi.attrs:\n",
    "            # Check for attributes like 'image1', 'image2', etc.\n",
    "            for attr_name, attr_value in gdi.attrs.items():\n",
    "                if 'image' in attr_name.lower():\n",
    "                    image_id = attr_name.replace('image', '')\n",
    "                    break\n",
    "\n",
    "        # Method 2: Extract from the tag string using regex\n",
    "        if not image_id:\n",
    "            tag_str = str(gdi)\n",
    "            match = re.search(r'<gdi\\s+image(\\d+)', tag_str)\n",
    "            if match:\n",
    "                image_id = match.group(1)\n",
    "\n",
    "        # Method 3: Fallback - use sequential numbering\n",
    "        if not image_id:\n",
    "            image_id = str(len(images) + 1)\n",
    "\n",
    "        content = gdi.get_text().strip()\n",
    "\n",
    "        # Extract tagged elements using BeautifulSoup directly\n",
    "        objects = [obj.get_text().strip() for obj in gdi.find_all('gdo')]\n",
    "        actions = [act.get_text().strip() for act in gdi.find_all('gda')]\n",
    "        locations = [loc.get_text().strip() for loc in gdi.find_all('gdl')]\n",
    "\n",
    "        images.append({\n",
    "            'image_id': image_id,\n",
    "            'description': content,\n",
    "            'objects': objects,\n",
    "            'actions': actions,\n",
    "            'locations': locations,\n",
    "            'raw_text': str(gdi)\n",
    "        })\n",
    "\n",
    "    return images\n",
    "\n",
    "# This is an utility function to show images.\n",
    "# Why do we need to do all this?\n",
    "def show_image(ax, image, de_normalize = False, img_mean = None, img_std = None):\n",
    "  \"\"\"\n",
    "  De-normalize the image (if necessary) and show image\n",
    "  \"\"\"\n",
    "  if de_normalize:\n",
    "    new_mean = -img_mean/img_std\n",
    "    new_std = 1/img_std\n",
    "\n",
    "    image = transforms.Normalize(\n",
    "        mean=new_mean,\n",
    "        std=new_std\n",
    "    )(image)\n",
    "  ax.imshow(image.permute(1, 2, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y2vjcpe_23w"
   },
   "source": [
    "Now we load dataset from HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "289ee05785d440e5977040c0c0afaa1c",
      "a399ae1787bb4ab58c5d2e9a414ee64e",
      "5be50764272342a087773846bcd8e39c",
      "66b3e60271574874a7026d984a828cac",
      "e62982be36574e34a65c9f043bc4df4d",
      "49cdf52254844e43820f1a371586ea99",
      "765a941f401444c185e88c666bc47ba9",
      "feac501030f1454a9b9e50ccfdb4e503",
      "e7fdb98f7fc84d6db012881ce75e491c",
      "7dfbd33db3ef44ae836a357827e60db2",
      "eed5a9dba7f14677ac2594069d46dfa3",
      "fae498ad859c4f8bb6b9fd85e4bd9378",
      "b425b1765dd1417a8f0eb1718fc4cbc5",
      "a210935a0462498b9210a240fe670734",
      "d06aef580e0f40608349b7a9fad4f1e9",
      "0ca0feeacba645abba84f48f59b0f6da",
      "297ac2254448428d963fc636f214e938",
      "ebc3248c5582406a985a7661de8c9685",
      "881119e930684fb8938928bce8754726",
      "6af2ab58357b41c48058dea9c1d22e7f",
      "7b7615bbc93743aab622c07be015474e",
      "0e8d75564aec4d86b350560342a0f990",
      "8df5d62cc8f34b5896e0ba54911c992c",
      "3c0d6bf24afb4c8c848247b45b270783",
      "d0422cb7923f4e82ad87dea110c19c37",
      "1a8f410b29d04fe6a27a5581185dbe82",
      "14b6cb319ab648b693413dc1589a03fa",
      "c20a7b4de11c47acaf9edf3260ef5f7b",
      "d86e423b04f74bb0aec81f82bd0c64b7",
      "f6fb36c8518c4799bf466d01dd282876",
      "77e5bde4daac4fbb99ffa3390599e457",
      "7faab64e03984559bc7f224eaa44f2bf",
      "d375dc1ec85b4defbe9d9cf8c8808958",
      "84da3c427cab4ca1b78f7cf384881323",
      "7a2f19c6ab574a679936ae47e8ddcecb",
      "12f4a89d1913442488f506508d93a8e5",
      "27c05586e30c4072a04bf219257fe02a",
      "aa6de166159e43b7b52a4981d09ddd6c",
      "cfcb1581bcc647eb84043d97e9a34176",
      "20166962a4fe49dd839bddb581dfb829",
      "20c509e24e204255abb6ea00fe9f5770",
      "88e193c5b4624677837d3b3f12c2f37a",
      "b8ef4e89253f4bca9c34fc2567614979",
      "6e2a466641e342f2907c84e5d186cb98",
      "334da70f625e4a359bdea636038b23df",
      "1c7e22a478e8412e86e0e2adcac039b9",
      "f548ca05b091437399f64bc9621d9214",
      "7f1be3153a1b4b98939e83f187c16db4",
      "6aae2bb65184415ca57d4bd2908fb880",
      "23ebe9046bec4be7bb4b67a897fe2774",
      "d2b1a1fd2efd4946973abb84736e5f56",
      "27e0970e89a74ca3be5b7f353e1b84a7",
      "563a0d9d6fd3418480c2d559dcea2732",
      "f940bfda65344259b487319055b71827",
      "de6560a8e7cf45cabb1a5c5acd4c7976",
      "30339a4d1b3d45dabf7f8e1543d70179",
      "abbe210e40604e7a85ddcd724d9e0657",
      "a9b057dfe5114e9193683acfaf9bbf7f",
      "eb87819ea36f48e9b9f36742a719fee0",
      "e2c1db121fa248b68697f62b0e1abf30",
      "fc9d1bd952144e4aac7fc8ba04d6ac38",
      "b4f8c53f04814ecb8d850db6ba80ff8d",
      "33a32b6e93784ee081489b215df6738b",
      "c06e4f7efd9e402ebe3c6e2b5d67c90c",
      "0ac4534ae2664d23a094ea9e7bbdaa19",
      "ad6791d88cd0474c91a0dca2875a8f7c"
     ]
    },
    "executionInfo": {
     "elapsed": 14198,
     "status": "ok",
     "timestamp": 1763392139213,
     "user": {
      "displayName": "ENIOLA AKINLUA",
      "userId": "13807007961803944509"
     },
     "user_tz": 0
    },
    "id": "EmfKGQWjZTIF",
    "outputId": "6ca48c6e-801d-417c-8e41-1b705ae11d1d"
   },
   "outputs": [],
   "source": [
    "# @title Loading the dataset\n",
    "train_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"train\")\n",
    "test_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIdO0M6JAf8A"
   },
   "source": [
    "In a previous lab, we analyzed the statistics of the images and figure out that a size of 240x500 could be good enough to standarize, but we will use 60x125. Also, we will restrict ourselves to taking only 5 frames from all the sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHKYBT5Fc3iM"
   },
   "source": [
    "## 1.2 Three datasets\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We will create three different dataset objects and the corresponding loaders for performing multiple tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WQIDYHGO1yN"
   },
   "outputs": [],
   "source": [
    "# @title Main dataset\n",
    "class SequencePredictionDataset(Dataset):\n",
    "    def __init__(self, original_dataset, tokenizer):\n",
    "        super(SequencePredictionDataset, self).__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        # Potential experiments: Try other transforms!\n",
    "        self.transform = transforms.Compose([\n",
    "          transforms.Resize((60, 125)),# Reasonable size based on our previous analysis\n",
    "          transforms.ToTensor(), # HxWxC -> CxHxW\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \"\"\"\n",
    "      Selects a 5 frame sequence from the dataset. Sets 4 for training and the last one\n",
    "      as a target.\n",
    "      \"\"\"\n",
    "      num_frames = self.dataset[idx][\"frame_count\"]\n",
    "      frames = self.dataset[idx][\"images\"]\n",
    "      self.image_attributes = parse_gdi_text(self.dataset[idx][\"story\"])\n",
    "\n",
    "      frame_tensors = []\n",
    "      description_list = []\n",
    "\n",
    "      for frame_idx in range(4):\n",
    "        image = FT.equalize(frames[frame_idx])\n",
    "        input_frame = self.transform(image)\n",
    "        frame_tensors.append(input_frame)\n",
    "\n",
    "        # Potential experiments: Try using the other attributes in your training\n",
    "        # objects = self.image_attributes[frame_idx][\"objects\"]\n",
    "        # actions = self.image_attributes[frame_idx][\"actions\"]\n",
    "        # locations = self.image_attributes[frame_idx][\"locations\"]\n",
    "\n",
    "        description = self.image_attributes[frame_idx][\"description\"]\n",
    "        # We need to return the tokens for NLP\n",
    "        input_ids =  self.tokenizer(description,\n",
    "                             return_tensors=\"pt\",\n",
    "                             padding=\"max_length\",\n",
    "                             truncation=True,\n",
    "                             max_length=120).input_ids\n",
    "\n",
    "        description_list.append(input_ids.squeeze(0))\n",
    "\n",
    "\n",
    "      image_target = FT.equalize(frames[4])\n",
    "      image_target = self.transform(image_target)\n",
    "      text_target = self.image_attributes[4][\"description\"]\n",
    "\n",
    "      target_ids = tokenizer(description,\n",
    "                             return_tensors=\"pt\",\n",
    "                             padding=\"max_length\",\n",
    "                             truncation=True,\n",
    "                             max_length=120).input_ids\n",
    "\n",
    "      sequence_tensor = torch.stack(frame_tensors)  # shape: (num_frames, C, H, W)\n",
    "      description_tensor = torch.stack(description_list) # (num_frames, max_length)\n",
    "\n",
    "      return (sequence_tensor, # Returning the image\n",
    "              description_tensor, # Returning the whole description\n",
    "              image_target, # Image target\n",
    "              target_ids) # Text target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haK3Fr2lezow"
   },
   "source": [
    "We will use text autoencoding (reconstructing the same text) to develop representations of the text (I provide some existing weights for this, but you can train your own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO2a-75Xeq24"
   },
   "outputs": [],
   "source": [
    "# @title Text task dataset (text autoencoding)\n",
    "class TextTaskDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      num_frames = self.dataset[idx][\"frame_count\"]\n",
    "      self.image_attributes = parse_gdi_text(self.dataset[idx][\"story\"])\n",
    "\n",
    "      # Pick\n",
    "      frame_idx = np.random.randint(0, 5)\n",
    "      description = self.image_attributes[frame_idx][\"description\"]\n",
    "\n",
    "      return description  # Returning the whole description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fp-A1oWfN46"
   },
   "source": [
    "And also a dataset for a potential image autoencoder task if you want to develop some visual features before training the whose archicture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgXo520EfdvG"
   },
   "outputs": [],
   "source": [
    "# @title Dataset for image autoencoder task\n",
    "class AutoEncoderTaskDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transforms.Compose([\n",
    "          transforms.Resize((240, 500)),# Reasonable size based on our previous analysis\n",
    "          transforms.ToTensor(), # HxWxC -> CxHxW\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      num_frames = self.dataset[idx][\"frame_count\"]\n",
    "      frames = self.dataset[idx][\"images\"]\n",
    "\n",
    "      # Pick a frame at random\n",
    "      frame_idx = torch.randint(0, num_frames-1, (1,)).item()\n",
    "      input_frame = self.transform(frames[frame_idx]) # Input to the autoencoder\n",
    "\n",
    "      return input_frame, # Returning the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5Wwza9Lf5IB"
   },
   "source": [
    "## 1.3 Creating and testing our dataset objects and loaders\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "29bb73bc68134d0db7f2ddf7100b29c5",
      "af6da2df309241608d713a29e77bd89f",
      "b174a2f07c8b49f5874d75e3ab69e682",
      "1467a8f9addd41ab81223e2dc4d79594",
      "7942d8f8ec3c48bdae3f15a430ed29b5",
      "9d9d6f6d7ad24d599317e985a9395c56",
      "e65fab64d26f4e5db0c8734b92261a77",
      "74447db795de4d2fbdff26f55b9a2463",
      "82f33b3652624b46b4858f982a2935de",
      "44e3b471aeb141a19e7e9c92a4ae00d5",
      "a2bda865d7a743bebaa1ce3bf98d5d45",
      "87ab058e5b3b48c086450eea1b59bead",
      "444abb4cf753495298039619046e107f",
      "f27016f6fe24427f90a7d4c2b64135e7",
      "608f7ffa1ef540578dd81e14848e387c",
      "0f779240b2184b069d7c5581e9ee126c",
      "f00422f54e5e41cfb1cfb4d88d23f65e",
      "89cf0aec628849efafe5522598d00da2",
      "8c1de28b30044aef8d3942c57127f5fe",
      "7c526b7c6a8b47a78334f950ed5f5e99",
      "977045212b8b41d790cba31a7dceddb0",
      "690921d79c27476fa04fc431e03b30f8",
      "09e75909ce434d71bb88ff642a3fde35",
      "848e37a83c574c8d99677f242c46c998",
      "b26ce156f00c47e9acaa7785bcbd02c2",
      "b8d30c3454e14be687e214d2775ea813",
      "dd0c26d02ae44950832f16b45dd3c940",
      "55addb2ae5c94a69872c3cca97c83151",
      "792938148b5d4b69ba69256c110048e5",
      "96295050e0f240ec8d91a9629a14366d",
      "59395ac46eb24754afcc1d814a64a7ca",
      "9913abc45098487481db02ccb030143e",
      "1f04e92e99054dffad4a0dfd0d7347d5",
      "7ef9d4e8652146209f3391ee8b842fe4",
      "f979789cc19346f5b838448f1b9bde7c",
      "ae8933cc527043c9b0587956eadfbea3",
      "0776b452b30e40a68063f078f823312a",
      "c2be08ba52194d17b9ceaa93f4742f9a",
      "ecedf8a626ff4df1b3182a3611b4128c",
      "e5d3df87c0544dbc845ac7737eb7ca9f",
      "e096922a006a4a2bb7f47782ad943435",
      "10e035e2045b4fa59b0582c4e82fea31",
      "d2f003b9f953429f9962db4ae69abc33",
      "c544df1cc1ab481ea72675fed1ff25e8"
     ]
    },
    "executionInfo": {
     "elapsed": 2430,
     "status": "ok",
     "timestamp": 1763392141705,
     "user": {
      "displayName": "ENIOLA AKINLUA",
      "userId": "13807007961803944509"
     },
     "user_tz": 0
    },
    "id": "KXqNDntLeoXb",
    "outputId": "a18657de-c9e3-4245-c1ae-a2c838da12b3"
   },
   "outputs": [],
   "source": [
    "# @title For the Sequence prediction task\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
    "sp_train_dataset = SequencePredictionDataset(train_dataset, tokenizer) # Instantiate the train dataset\n",
    "sp_test_dataset = SequencePredictionDataset(test_dataset, tokenizer) # Instantiate the test dataset\n",
    "\n",
    "# Let's do things properly, we will also have a validation split\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(sp_train_dataset))\n",
    "val_size = len(sp_train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(sp_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Instantiate the dataloaders\n",
    "train_dataloader = DataLoader(train_subset, batch_size=8, shuffle=True)\n",
    "# We will use the validation set to visualize the progress.\n",
    "val_dataloader = DataLoader(val_subset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(sp_test_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vHNzyhvg82K"
   },
   "outputs": [],
   "source": [
    "# @title For the text task\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
    "text_dataset = TextTaskDataset(train_dataset)\n",
    "text_dataloader = DataLoader(text_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dkjd6ZiFhmL5"
   },
   "outputs": [],
   "source": [
    "# @title For the image autoencoder task\n",
    "autoencoder_dataset = AutoEncoderTaskDataset(train_dataset)\n",
    "autoencoder_dataloader = DataLoader(autoencoder_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1763392404815,
     "user": {
      "displayName": "ENIOLA AKINLUA",
      "userId": "13807007961803944509"
     },
     "user_tz": 0
    },
    "id": "knyhiCkWgtGS",
    "outputId": "795492f4-2b7c-4aa3-ee70-420da22b3c0d"
   },
   "outputs": [],
   "source": [
    "# @title Testing some of the outputs of the SP dataset\n",
    "frames, descriptions, image_target, text_target = sp_train_dataset[np.random.randint(0,400)]\n",
    "\n",
    "print(\"Description: \", descriptions.shape)\n",
    "figure, ax = plt.subplots(1,1)\n",
    "show_image(ax, image_target)\n",
    "\n",
    "# Do some tests on the batches (try with batch size small)\n",
    "frames, descriptions, image_target, text_target = next(iter(train_dataloader))\n",
    "print(frames.shape)\n",
    "print(descriptions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kOXysg0hvFA"
   },
   "source": [
    "I will leave the test of the other datasets to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9XXLlboqSZk"
   },
   "source": [
    "# **Chapter 2: Models**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Jr15QK1iltL"
   },
   "source": [
    "We provide a simple text encoder based on a recurrent neural network (LSTM). Feel free to provide your own text encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDGTd0lYj1ZF"
   },
   "source": [
    "## 2.1 The NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "clpqOq49cV4H"
   },
   "outputs": [],
   "source": [
    "# @title The text autoencoder (Seq2Seq)\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "      Encodes a sequence of tokens into a latent space representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "      Decodes a latent space representation into a sequence of tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size) # Should be hidden_dim\n",
    "\n",
    "    def forward(self, input_seq, hidden, cell):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.out(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# We create the basic text autoencoder (a special case of a sequence to sequence model)\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        # input_seq and target_seq are both your 'input_ids'\n",
    "        # 1. Encode the input sequence\n",
    "        _enc_out, hidden, cell = self.encoder(input_seq)\n",
    "\n",
    "        # 2. Create the \"shifted\" decoder input for teacher forcing.\n",
    "        # We want to predict target_seq[:, 1:]\n",
    "        # So, we feed in target_seq[:, :-1]\n",
    "        # (i.e., feed \"[SOS], hello, world\" to predict \"hello, world, [EOS]\")\n",
    "        decoder_input = target_seq[:, :-1]\n",
    "\n",
    "        # 3. Run the decoder *once* on the entire sequence.\n",
    "        # It takes the encoder's final state (hidden, cell)\n",
    "        # and the full \"teacher\" sequence (decoder_input).\n",
    "        predictions, _hidden, _cell = self.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "        # predictions shape will be (batch_size, seq_len-1, vocab_size)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vmA3j5jmN_S6"
   },
   "outputs": [],
   "source": [
    "# @title Utility functions for NLP tasks\n",
    "def generate(model, hidden, cell, max_len, sos_token_id, eos_token_id):\n",
    "      \"\"\"\n",
    "        This function generates a sequence of tokens using the provided decoder.\n",
    "      \"\"\"\n",
    "      # Ensure the model is in evaluation mode\n",
    "      model.eval()\n",
    "\n",
    "      # 2. SETUP DECODER INPUT\n",
    "      # Start with the SOS token, shape (1, 1)\n",
    "      dec_input = torch.tensor([[sos_token_id]], dtype=torch.long, device=device)\n",
    "      # hidden = torch.zeros(1, 1, hidden_dim, device=device)\n",
    "      # cell = torch.zeros(1, 1, hidden_dim, device=device)\n",
    "\n",
    "      generated_tokens = []\n",
    "\n",
    "      # 3. AUTOREGRESSIVE LOOP\n",
    "      for _ in range(max_len):\n",
    "          with torch.no_grad():\n",
    "              # Run the decoder one step at a time\n",
    "              # dec_input is (1, 1) hereit's just the last predicted token\n",
    "              prediction, hidden, cell = model(dec_input, hidden, cell)\n",
    "\n",
    "          logits = prediction.squeeze(1) # Shape (1, vocab_size)\n",
    "          temperature = 0.9 # <--- Try a value between 0.5 and 1.0\n",
    "\n",
    "          # 1. Divide logits by temperature\n",
    "          # 2. Apply softmax to get probabilities\n",
    "          # 3. Use multinomial to sample one token based on the probabilities\n",
    "          probabilities = torch.softmax(logits / temperature, dim=-1)\n",
    "          next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "          token_id = next_token.squeeze().item()\n",
    "\n",
    "          # Check for the End-of-Sequence token\n",
    "          if token_id == eos_token_id:\n",
    "              break\n",
    "\n",
    "          if token_id == 0 or token_id == sos_token_id:\n",
    "              continue\n",
    "\n",
    "            # Append the predicted token\n",
    "          generated_tokens.append(token_id)\n",
    "\n",
    "          # The predicted token becomes the input for the next iteration\n",
    "          dec_input = next_token\n",
    "\n",
    "      # Return the list of generated token IDs\n",
    "      return generated_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Zed8_Mk5jt3S"
   },
   "outputs": [],
   "source": [
    "# @title Do some tests\n",
    "# desc = text_dataset[np.random.randint(0, 100)]\n",
    "# print(f\"Input: {desc}\")\n",
    "# input_ids = tokenizer(desc, return_tensors=\"pt\",  padding=True, truncation=True).input_ids\n",
    "# input_ids = input_ids.to(device)\n",
    "# generated_tokens = generate(model, hidden, cell, max_len=100, sos_token_id=tokenizer.cls_token_id, eos_token_id=tokenizer.sep_token_id)\n",
    "# print(\"Output: \", tokenizer.decode(generated_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmm8U_UFkGQJ"
   },
   "source": [
    "## 2.2 The Vision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WU3lP9nFftc9"
   },
   "outputs": [],
   "source": [
    "# @title The visual autoencoder\n",
    "class Backbone(nn.Module):\n",
    "    \"\"\"\n",
    "      Main convolutional blocks for our CNN\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n",
    "        super(Backbone, self).__init__()\n",
    "        # Encoder convolutional layers\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 7, stride=2, padding=3),\n",
    "            nn.GroupNorm(8, 16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.Conv2d(16, 32, 5, stride=2, padding=2),\n",
    "            nn.GroupNorm(8, 32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "        # Calculate flattened dimension for linear layer\n",
    "        self.flatten_dim = 64 * output_w * output_h\n",
    "        # Latent space layers\n",
    "        self.fc1 = nn.Sequential(nn.Linear(self.flatten_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_conv(x)\n",
    "        x = x.view(-1, self.flatten_dim)  # flatten for linear layer\n",
    "        z = self.fc1(x)\n",
    "        return z\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "      Encodes an image into a latent space representation. Note the two pathways\n",
    "      to try to disentangle the mean pattern from the image\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n",
    "        super(VisualEncoder, self).__init__()\n",
    "\n",
    "        self.context_backbone = Backbone(latent_dim, output_w, output_h)\n",
    "        self.content_backbone = Backbone(latent_dim, output_w, output_h)\n",
    "\n",
    "        self.projection = nn.Linear(2*latent_dim, latent_dim)\n",
    "    def forward(self, x):\n",
    "        z_context = self.context_backbone(x)\n",
    "        z_content = self.content_backbone(x)\n",
    "        z = torch.cat((z_content, z_context), dim=1)\n",
    "        z = self.projection(z)\n",
    "        return z\n",
    "\n",
    "class VisualDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "      Decodes a latent representation into a content image and a context image\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n",
    "        super(VisualDecoder, self).__init__()\n",
    "        self.imh = 60\n",
    "        self.imw = 125\n",
    "        self.flatten_dim = 64 * output_w * output_h\n",
    "        self.output_w = output_w\n",
    "        self.output_h = output_h\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_dim, self.flatten_dim)\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "          nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=(1,1)),\n",
    "          nn.GroupNorm(8, 32),\n",
    "          nn.LeakyReLU(0.1),\n",
    "\n",
    "          nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "          nn.GroupNorm(8, 16),\n",
    "          nn.LeakyReLU(0.1),\n",
    "\n",
    "          nn.ConvTranspose2d(16, 3, kernel_size=7, stride=2, padding=3, output_padding=(1, 1)),\n",
    "          nn.Sigmoid() # Use nn.Tanh() if your data is normalized to [-1, 1]\n",
    "      )\n",
    "\n",
    "    def forward(self, z):\n",
    "      x = self.fc1(z)\n",
    "\n",
    "      x_content = self.decode_image(x)\n",
    "      x_context = self.decode_image(x)\n",
    "\n",
    "      return x_content, x_context\n",
    "\n",
    "    def decode_image(self, x):\n",
    "      x = x.view(-1, 64, self.output_w, self.output_h)      # reshape to conv feature map\n",
    "      x = self.decoder_conv(x)\n",
    "      x = x[:, :, :self.imh, :self.imw]          # crop to original size if needed\n",
    "      return x\n",
    "\n",
    "class VisualAutoencoder( nn.Module):\n",
    "    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n",
    "        super(VisualAutoencoder, self).__init__()\n",
    "        self.encoder = VisualEncoder(latent_dim, output_w, output_h)\n",
    "        self.decoder = VisualDecoder(latent_dim, output_w, output_h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalVisualDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16, output_w=8, output_h=16):\n",
    "        super(VariationalVisualDecoder, self).__init__()\n",
    "        self.imh = 60\n",
    "        self.imw = 125\n",
    "        self.flatten_dim = 64 * output_w * output_h\n",
    "        self.output_w = output_w\n",
    "        self.output_h = output_h\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc_mean = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, self.flatten_dim)\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=(1,1)),\n",
    "            nn.GroupNorm(8, 32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.GroupNorm(8, 16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=7, stride=2, padding=3, output_padding=(1, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar) \n",
    "        eps = torch.randn_like(std)\n",
    "        z_sampled = mean + eps * std \n",
    "        return z_sampled\n",
    "\n",
    "    def forward(self, z):\n",
    "        mean = self.fc_mean(z)    \n",
    "        logvar = self.fc_logvar(z) \n",
    "        \n",
    "        z_sampled = self.reparameterize(mean, logvar)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)\n",
    "        kl_loss = kl_loss.mean()\n",
    "        \n",
    "        \n",
    "        x = self.fc1(z_sampled)\n",
    "        x_content = self.decode_image(x)\n",
    "        x_context = self.decode_image(x)\n",
    "        \n",
    "        return x_content, x_context, kl_loss\n",
    "\n",
    "    def decode_image(self, x):\n",
    "        x = x.view(-1, 64, self.output_w, self.output_h)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = x[:, :, :self.imh, :self.imw]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxpXdi8ukzaQ"
   },
   "source": [
    "## 2.3 The main architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LqesywR-k-M-"
   },
   "outputs": [],
   "source": [
    "# @title A simple attention architecture\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        # This \"attention\" layer learns a query vector\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim=1) # Over the sequence length\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        # rnn_outputs shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Pass through linear layer to get \"energy\" scores\n",
    "        energy = self.attn(rnn_outputs).squeeze(2) # Shape: [batch, seq_len]\n",
    "\n",
    "        # Get attention weights\n",
    "        attn_weights = self.softmax(energy) # Shape: [batch, seq_len]\n",
    "\n",
    "        # Apply weights\n",
    "        # attn_weights.unsqueeze(1) -> [batch, 1, seq_len]\n",
    "        # bmm with rnn_outputs -> [batch, 1, hidden_dim]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs)\n",
    "\n",
    "        # Squeeze to get final context vector\n",
    "        return context.squeeze(1) # Shape: [batch, hidden_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Modal Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalAttention(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.scale = 1.0 / (hidden_dim ** 0.5)\n",
    "\n",
    "    def forward(self, visual_features, text_features):\n",
    "        Q = self.query_proj(visual_features)\n",
    "        K = self.key_proj(text_features)\n",
    "        V = self.value_proj(text_features)\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) * self.scale\n",
    "        \n",
    "        attn_weights = self.softmax(scores)\n",
    "        context = torch.bmm(attn_weights, V)\n",
    "        fused = visual_features + context\n",
    "        \n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pOQwqPPpk6pP"
   },
   "outputs": [],
   "source": [
    "# @title The main sequence predictor model\n",
    "\n",
    "class SequencePredictor(nn.Module):\n",
    "    def __init__(self, visual_autoencoder, text_autoencoder, latent_dim,\n",
    "                 gru_hidden_dim, use_cross_modal_attention=False, use_vae_decoder=False):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "\n",
    "        # --- 1. Static Encoders ---\n",
    "        # (These process one pair at a time)\n",
    "        self.use_cross_modal = use_cross_modal_attention\n",
    "        self.use_vae = use_vae_decoder\n",
    "        self.image_encoder = visual_autoencoder.encoder\n",
    "        self.text_encoder = text_autoencoder.encoder\n",
    "\n",
    "        # --- 2. Temporal Encoder ---\n",
    "        # (This processes the sequence of pairs)\n",
    "        fusion_dim = latent_dim * 2 # z_visual + z_text\n",
    "        self.temporal_rnn = nn.GRU(fusion_dim, latent_dim, batch_first=True)\n",
    "\n",
    "        # --- 3. Attention ---\n",
    "        self.attention = Attention(gru_hidden_dim)\n",
    "\n",
    "        if use_cross_modal_attention:\n",
    "            self.cross_modal_attention = CrossModalAttention(latent_dim)\n",
    "\n",
    "        # --- 4. Final Projection ---\n",
    "        # cat(h, context) -> gru_hidden_dim * 2\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_dim * 2, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # --- 5. Decoders ---\n",
    "        # (These predict the *next* item)\n",
    "        self.image_decoder = visual_autoencoder.decoder\n",
    "        self.text_decoder = text_autoencoder.decoder\n",
    "\n",
    "        self.fused_to_h0 = nn.Linear(latent_dim, 16)\n",
    "        self.fused_to_c0 = nn.Linear(latent_dim, 16)\n",
    "\n",
    "    def forward(self, image_seq, text_seq, target_seq):\n",
    "        # image_seq shape: [batch, seq_len, C, H, W]\n",
    "        # text_seq shape:  [batch, seq_len, text_len]\n",
    "        # target_text_for_teacher_forcing: [batch, text_len] (This is the last text)\n",
    "\n",
    "        batch_size, seq_len, C, H, W = image_seq.shape\n",
    "\n",
    "        # --- 1 & 2: Run Static Encoders over the sequence ---\n",
    "        # We can't pass a 5D/4D tensor to the encoders.\n",
    "        # We \"flatten\" the batch and sequence dimensions.\n",
    "\n",
    "        # Reshape for image_encoder\n",
    "        img_flat = image_seq.view(batch_size * seq_len, C, H, W)\n",
    "        # Reshape for text_encoder\n",
    "        txt_flat = text_seq.view(batch_size * seq_len, -1) # -1 infers text_len\n",
    "\n",
    "        # Run encoders\n",
    "        z_v_flat = self.image_encoder(img_flat) # Shape: [b*s, latent]\n",
    "        _, hidden, cell = self.text_encoder(txt_flat) # Shape: [b*s, latent]\n",
    "\n",
    "        # Combine\n",
    "        z_fusion_flat = torch.cat((z_v_flat, hidden.squeeze(0)), dim=1) # Shape: [b*s, fusion_dim]\n",
    "\n",
    "        # \"Un-flatten\" back into a sequence\n",
    "        z_fusion_seq = z_fusion_flat.view(batch_size, seq_len, -1) # Shape: [b, s, fusion_dim]\n",
    "\n",
    "        # --- 3. Run Temporal Encoder ---\n",
    "        # zseq shape: [b, s, gru_hidden]\n",
    "        # h    shape: [1, b, gru_hidden]\n",
    "        zseq, h = self.temporal_rnn(z_fusion_seq)\n",
    "        h = h.squeeze(0) # Shape: [b, gru_hidden]\n",
    "\n",
    "        # --- 4. Attention ---\n",
    "        context = self.attention(zseq) # Shape: [b, gru_hidden]\n",
    "\n",
    "        # --- 5. Final Prediction Vector (z) ---\n",
    "        z = self.projection(torch.cat((h, context), dim=1)) # Shape: [b, joint_latent_dim]\n",
    "\n",
    "        # --- 6. Decode (Predict pk) ---\n",
    "        pred_image_content, pred_image_context = self.image_decoder(z)\n",
    "\n",
    "        h0 = self.fused_to_h0(z).unsqueeze(0)\n",
    "        c0 = self.fused_to_c0(z).unsqueeze(0)\n",
    "\n",
    "        decoder_input = target_seq[:, :,:-1].squeeze(1)\n",
    "\n",
    "        # 3. Run the decoder *once* on the entire sequence.\n",
    "        # It takes the encoder's final state (hidden, cell)\n",
    "        # and the full \"teacher\" sequence (decoder_input).\n",
    "        predicted_text_logits_k, _hidden, _cell = self.text_decoder(decoder_input, h0, c0)\n",
    "\n",
    "        return pred_image_content, pred_image_context, predicted_text_logits_k,h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BuFpu92fz1V"
   },
   "source": [
    "# **Chapter 3: Training routines**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ySQ7jC3dEpUm"
   },
   "outputs": [],
   "source": [
    "# @title Training utility functions: To initialize and to visualize the progress\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Plots four images and their reconstructions\n",
    "def validation( model, data_loader ):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    frames, descriptions, image_target, text_target = next(iter(data_loader))\n",
    "\n",
    "    descriptions = descriptions.to(device)\n",
    "    frames = frames.to(device)\n",
    "    image_target = image_target.to(device)\n",
    "    text_target = text_target.to(device)\n",
    "\n",
    "    predicted_image_k,context_image, _, hidden, cell = model(frames, descriptions, text_target)\n",
    "\n",
    "    figure, ax = plt.subplots(2, 6, figsize=(20, 5), gridspec_kw={'height_ratios': [2, 1.5]})\n",
    "\n",
    "    for i in range(4):\n",
    "      im = frames[0, i, :, :, :].cpu()\n",
    "      show_image(ax[0,i], im )\n",
    "      ax[0,i].set_aspect('auto')\n",
    "      ax[0,i].axis('off')\n",
    "      wrapped_text = textwrap.fill(tokenizer.decode(descriptions[0, i, :], skip_special_tokens=True), width=40)\n",
    "\n",
    "      ax[1,i].text(\n",
    "            0.5, 0.99,\n",
    "            wrapped_text,\n",
    "            ha='center',\n",
    "            va='top',\n",
    "            fontsize=10,\n",
    "            wrap=True\n",
    "        )\n",
    "\n",
    "      ax[1,i].axis('off') # Hide axes for the text subplot\n",
    "\n",
    "    show_image(ax[0,4], image_target[0].cpu())\n",
    "    ax[0,4].set_title('Target')\n",
    "    ax[0,4].set_aspect('auto')\n",
    "    ax[0,4].axis('off')\n",
    "    text_target = text_target.squeeze(1)\n",
    "\n",
    "    wrapped_text = textwrap.fill(tokenizer.decode(text_target[0], skip_special_tokens=True), width=40)\n",
    "    ax[1,4].text(\n",
    "            0.5, 0.99,\n",
    "            wrapped_text,\n",
    "            ha='center',\n",
    "            va='top',\n",
    "            fontsize=10,\n",
    "            wrap=False)\n",
    "    ax[1,4].axis('off')\n",
    "    output = context_image[0, :, :, :].cpu()\n",
    "    show_image(ax[0,5], output)\n",
    "    ax[0,5].set_title('Predicted')\n",
    "    ax[0,5].set_aspect('auto')\n",
    "    ax[0,5].axis('off')\n",
    "\n",
    "    generated_tokens = generate(model.text_decoder,\n",
    "                                hidden[:,0, :].unsqueeze(1),\n",
    "                                cell[:, 0, :].unsqueeze(1),\n",
    "                                max_len=150,\n",
    "                                sos_token_id=tokenizer.cls_token_id,\n",
    "                                eos_token_id=tokenizer.sep_token_id)\n",
    "\n",
    "    wrapped_text = textwrap.fill(tokenizer.decode(generated_tokens), width=40)\n",
    "\n",
    "    ax[1,5].text(\n",
    "            0.5, 0.99,\n",
    "            wrapped_text,\n",
    "            ha='center',\n",
    "            va='top',\n",
    "            fontsize=10,\n",
    "            wrap=False )\n",
    "    ax[1,5].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQKrCAuxrtIq"
   },
   "source": [
    "## 3.1 Initialization and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JspxI_aNmtni"
   },
   "outputs": [],
   "source": [
    "# @title Variables and initial setup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N_EPOCHS = 5\n",
    "emb_dim = 16\n",
    "latent_dim = 16\n",
    "num_layers = 1\n",
    "dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1763394285556,
     "user": {
      "displayName": "ENIOLA AKINLUA",
      "userId": "13807007961803944509"
     },
     "user_tz": 0
    },
    "id": "flzU2iConBZm",
    "outputId": "198fbf07-2394-4512-eeb6-41e814daab11"
   },
   "outputs": [],
   "source": [
    "# @title Initializing the NLP models\n",
    "encoder = EncoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)\n",
    "decoder = DecoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)\n",
    "text_autoencoder = Seq2SeqLSTM(encoder, decoder).to(device)\n",
    "text_autoencoder, _, _, _ = load_checkpoint_from_drive(text_autoencoder, None, filename='text_autoencoder.pth')\n",
    "\n",
    "total_params = sum(p.numel() for p in text_autoencoder.parameters())\n",
    "print(f\"Total parameters (Not trainable): {total_params}\")\n",
    "# Deactivating training from this model for efficiency (although not ideal)\n",
    "for param in text_autoencoder.parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1763394289623,
     "user": {
      "displayName": "ENIOLA AKINLUA",
      "userId": "13807007961803944509"
     },
     "user_tz": 0
    },
    "id": "oIMofjnFnU03",
    "outputId": "676cd6cd-3b6d-4e8f-9ae3-75926d0f04f3"
   },
   "outputs": [],
   "source": [
    "# @title Initializing visual models\n",
    "visual_autoencoder = VisualAutoencoder(latent_dim=16)\n",
    "visual_autoencoder.apply(init_weights)\n",
    "\n",
    "total_params = sum(p.numel() for p in visual_autoencoder.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters in visual autoencoder: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1763394290683,
     "user": {
      "displayName": "ENIOLA AKINLUA",
      "userId": "13807007961803944509"
     },
     "user_tz": 0
    },
    "id": "eY5uwVbcnf6B",
    "outputId": "1f72829b-a0c7-48e5-ff1b-c5717a968b27"
   },
   "outputs": [],
   "source": [
    "# @title Initialize the main architecture\n",
    "# We put all the sizes the same, not ideal as well\n",
    "sequence_predictor = SequencePredictor(visual_autoencoder, text_autoencoder, latent_dim, latent_dim)\n",
    "sequence_predictor.to(device)\n",
    "\n",
    "# # Print number of trainable parameters\n",
    "total_params = sum(p.numel() for p in sequence_predictor.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters in the whole model: {total_params}\")\n",
    "\n",
    "# Print model size\n",
    "total_params = sum(p.numel() for p in sequence_predictor.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL2e-u8VrzH6"
   },
   "source": [
    "## 3.2 Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VqQobZRwnxt1"
   },
   "outputs": [],
   "source": [
    "# @title Training tools\n",
    "criterion_images = nn.L1Loss()\n",
    "criterion_ctx = nn.MSELoss()\n",
    "criterion_text = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
    "optimizer = torch.optim.Adam(sequence_predictor.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLu8BZkDCup3"
   },
   "outputs": [],
   "source": [
    "# @title Training loop for the sequence predictor\n",
    "# Instantiate the model, define loss and optimizer\n",
    "\n",
    "sequence_predictor.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for frames, descriptions, image_target, text_target  in train_dataloader:\n",
    "\n",
    "      # Send images and tokens to the GPU\n",
    "      descriptions = descriptions.to(device)\n",
    "      frames = frames.to(device)\n",
    "      image_target = image_target.to(device)\n",
    "      text_target = text_target.to(device)\n",
    "      # Predictions from our model\n",
    "      pred_image_content, pred_image_context, predicted_text_logits_k, _, _ = sequence_predictor(frames, descriptions, text_target)\n",
    "      # Computing losses\n",
    "      # Loss for image reconstruction\n",
    "      loss_im = criterion_images(pred_image_content, image_target)\n",
    "      # Loss for the average pattern the images contain\n",
    "      mu_global = frames.mean(dim=[0, 1])\n",
    "      mu_global = mu_global.unsqueeze(0).expand_as(pred_image_context)\n",
    "      loss_context = criterion_ctx(pred_image_context, mu_global)\n",
    "      # Loss function for the text prediction\n",
    "      prediction_flat = predicted_text_logits_k.reshape(-1, tokenizer.vocab_size)\n",
    "      target_labels = text_target.squeeze(1)[:, 1:] # Slice to get [8, 119]\n",
    "      target_flat = target_labels.reshape(-1)\n",
    "      loss_text = criterion_text(prediction_flat, target_flat)\n",
    "      # Combining the losses\n",
    "      loss = loss_im + loss_text + 0.2*loss_context\n",
    "      # Optimizing\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item() * frames.size(0)\n",
    "\n",
    "    # checking model performance on validation set\n",
    "    sequence_predictor.eval()\n",
    "    print(\"Validation on training dataset\")\n",
    "    print( \"----------------\")\n",
    "    validation( sequence_predictor, train_dataloader )\n",
    "    print(\"Validation on validation dataset\")\n",
    "    print( \"----------------\")\n",
    "    validation( sequence_predictor, val_dataloader)\n",
    "    sequence_predictor.train()\n",
    "\n",
    "    # scheduler.step()\n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{N_EPOCHS}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      save_checkpoint_to_drive(sequence_predictor, optimizer, epoch, epoch_loss, filename=f\"sequence_predictor.pth\")\n",
    "\n",
    "# Do better plots\n",
    "plt.plot(losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNA34cwE6KKS"
   },
   "outputs": [],
   "source": [
    "# @title Example text reconstruction task\n",
    "\n",
    "# Don't forget to unfreeze the model!\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(text_autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    text_autoencoder.train()\n",
    "    epoch_loss = 0\n",
    "    for description in text_dataloader:\n",
    "        # Move the \"sentences\" to device\n",
    "        input_ids = tokenizer(description, return_tensors=\"pt\",  padding=True, truncation=True).input_ids\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # zero the grad, then forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = text_autoencoder(input_ids, input_ids)\n",
    "        # compute the loss: compare 3D logits to 2D targets\n",
    "        loss = loss_fn(outputs.reshape(-1, tokenizer.vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}; Avg loss {epoch_loss/len(text_dataloader)}; Latest loss {loss.item()}\")\n",
    "    torch.save(text_autoencoder.state_dict(), f\"seq2seq-epoch-{epoch+1}.pth\")\n",
    "\n",
    "# # saving checkpoint to drive\n",
    "save_checkpoint_to_drive(text_autoencoder, optimizer, 3*N_EPOCHS, loss, filename = \"text_autoencoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9UsFH4HpHyT"
   },
   "outputs": [],
   "source": [
    "# @title Image reonstruction task\n",
    "\n",
    "# To-Do: Use previous labs if you want to pretrain your visual encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjnJ5PyvpRFY"
   },
   "source": [
    "# **Appendix**\n",
    "\n",
    "This code computes the average images in case you want to use them. Notice that the average should be all zeros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArBv44uqvDYg"
   },
   "outputs": [],
   "source": [
    "# @title Computing and showing average images\n",
    "N = 1000\n",
    "H, W = 60, 125\n",
    "\n",
    "# Tensors to accumulate sum (for mean) and sum of squares (for variance)\n",
    "avg_images = [torch.zeros((3, H, W)) for _ in range(5)]\n",
    "sum_sq_diff = [torch.zeros((3, H, W)) for _ in range(5)] # Placeholder for variance numerator\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# --- First Pass: Calculate the Sum (for Mean) ---\n",
    "print(\"Starting Pass 1: Calculating Mean...\")\n",
    "\n",
    "for i in range(N):\n",
    "    # Process sequence i\n",
    "    sequence = train_dataset[i][\"images\"]\n",
    "\n",
    "    for j in range(5):\n",
    "        image = transform(sequence[j])\n",
    "        avg_images[j] += image # Sum for mean\n",
    "\n",
    "# Final step for mean\n",
    "for j in range(5):\n",
    "    avg_images[j] /= N\n",
    "\n",
    "print(\"Starting Pass 2: Calculating Variance...\")\n",
    "\n",
    "for i in range(N):\n",
    "    # Process sequence i\n",
    "    sequence = train_dataset[i][\"images\"]\n",
    "\n",
    "    for j in range(5):\n",
    "        image = transform(sequence[j])\n",
    "\n",
    "        # Calculate (Image - Mean)^2\n",
    "        # Note: We detach the mean from the computation graph if it were being trained,\n",
    "        # but here we're just using it as a fixed statistical value.\n",
    "        diff = image - avg_images[j]\n",
    "        sum_sq_diff[j] += diff * diff # Element-wise squaring\n",
    "\n",
    "# --- Final step for Standard Deviation ---\n",
    "std_images = []\n",
    "for j in range(5):\n",
    "    # Variance = Sum of Squared Differences / N\n",
    "    variance = sum_sq_diff[j] / N\n",
    "\n",
    "    # Standard Deviation = sqrt(Variance)\n",
    "    std_dev = torch.sqrt(variance)\n",
    "    std_images.append(std_dev)\n",
    "\n",
    "print(\"Computation Complete. std_images is a list of 5 tensors (3x60x125).\")\n",
    "# You now have the 5 tensors you need for normalization (mean and std).\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=(15,5))\n",
    "for i in range(5):\n",
    "  avg_image = avg_images[i]\n",
    "\n",
    "  # Printing range of avg_image\n",
    "  print(torch.min(avg_image), torch.max(avg_image))\n",
    "\n",
    "  avg_imagen = (avg_image - torch.min(avg_image))/(torch.max(avg_image) - torch.min(avg_image))\n",
    "  show_image(ax[i], avg_imagen)\n",
    "\n",
    "# Create a matrix of images with the differences between avg_images\n",
    "fig, ax = plt.subplots(5,5, figsize=(15,8))\n",
    "\n",
    "for i in range(5):\n",
    "  for j in range(5):\n",
    "    if i == j:\n",
    "      avg_image = avg_images[i]\n",
    "      avg_imagen = (avg_image - torch.min(avg_image))/(torch.max(avg_image) - torch.min(avg_image))\n",
    "      show_image(ax[i,j], avg_imagen)\n",
    "    else:\n",
    "      diff = avg_images[i] - avg_images[j]\n",
    "      diff = (diff - torch.min(diff))/(torch.max(diff) - torch.min(diff))\n",
    "      show_image(ax[i,j], diff)\n",
    "    ax[i,j].set_xticks([])\n",
    "    ax[i,j].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(\n",
    "    wspace=0, # Set horizontal space to zero\n",
    "    hspace=0  # Set vertical space to zero\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
